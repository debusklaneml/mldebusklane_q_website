---
title: "Stuff I Use 001: Sentence Embeddings for Classification"
description: "Stuff I Use is a blog series where I try to document advanced cool or interesting methods that I get to use on a day to day basis."
date: 06-15-2024
categories: [R, Python, Transformers, NLP] # self-defined categories
citation: 
  url: https://www.mldebusklane.com/posts/siu/siu_001.html

output: 
  html_document:
    theme: journal
    highlight: zenburn
    
draft: false
---

# Why:

I often have a column of text responses that need to be classified. Essentially, this involves a zero-shot approach where I use a sentence transformer to generate embeddings for each response. I do the same for each label used in the classification. By calculating a similarity metric between each row and label, often cosine similarity, we can determine which embeddings (vectors representing sentences in lexical space) are most closely related to the labels. 

## Setup:

### Load R Packages:
```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, here, pins, janitor, readxl,
               reticulate, gt)
```

### Set Up Functions and Python Env:
```{r}
#| eval: false
source(here('src', 'embed_process.R'))
use_condaenv(condaenv = 'embed', required = TRUE)
source_python(here('src', 'embed.py'))
```

A few notes: The `use_condaenv` is a pre-established conda environment that matches the required libraries/packages needed in your sourced py file. 

```{r}
#| echo: false
# source(here('src', 'embed_process.R'))
use_condaenv(condaenv = 'embed', required = TRUE)
source_python(here('src', 'embed.py'))
```

## Review Key Functions: 

This function is saved within the earlier sourced R script, `embed_process.R`.

In all there are three functions here. The overall management function, and two internals that compute cosign similarity and another that situates the matrices to compute the similarity scores on a row by row basis. 

Inputs:

	•	cats: Data frame containing the categories with their respective labels.
	•	input_data: Data frame containing the input data for which cosine similarity will be calculated.
	•	min_sim: Minimum similarity threshold (though not used directly in the provided function).
	•	target_verbatim: The column name in input_data containing the text data that will be used to generate embeddings.
	
```{r}
embedding_cosign <- function(cats, input_data, target_verbatim) {
  cats_emb <- add_embeddings_to_dataframe(cats, 'label')
  test_emb <- add_embeddings_to_dataframe(input_data, target_verbatim)
  
  compute_cosine_similarity <- function(vec1, vec2) {
    sum(vec1 * vec2) / (sqrt(sum(vec1^2)) * sqrt(sum(vec2^2)))
  }

  compare_cats_to_utterances <- function(cats_emb, data_emb) {
    # Extract embeddings into matrices
    cats_matrix <- as.matrix(cats_emb |> select(starts_with('embedding')))
    data_matrix <- as.matrix(data_emb |> select(starts_with('embedding')))

    # Compute cosine similarity
    similarity_scores <- map_df(1:nrow(data_matrix), function(i) {
      map_dfc(1:nrow(cats_matrix), function(j) {
        score <- compute_cosine_similarity(data_matrix[i,], cats_matrix[j,])
        col_name <- cats_emb[j, 1] # Direct use of the category label
        return(setNames(list(score), col_name))
      })
    })

    # Combine similarity scores with original utterance data (excluding embeddings)
    result_df <- bind_cols(data_emb |> select(-starts_with('embedding')), similarity_scores)

    return(result_df)
  }
  
  result <- compare_cats_to_utterances(
    cats_emb = cats_emb,
    data_emb = test_emb
  ) 
}
```

As mentioned earlier, you'll need that conda environment to get these libraries going.

This function is very simple. Input any dataframe and the column you want to generate embedding for. It outputs the same dataframe and the embedding columns. In this case, I'm using the `gte-base` model. You can check it out here to see how it compares to other text embedding models: https://huggingface.co/thenlper/gte-base
	
```{python}
#| eval: false
import pandas as pd
from sentence_transformers import SentenceTransformer

def add_embeddings_to_dataframe(df, column_name):
    # Load the sentence transformer model
    model = SentenceTransformer('thenlper/gte-base')
    
    # Ensure the specified column exists in the DataFrame
    if column_name not in df.columns:
        raise ValueError(f"Column '{column_name}' not found in DataFrame.")
    
    # Generate embeddings for the specified column.
    # The output is a list of lists (each inner list is an embedding vector for a sentence).
    embeddings = model.encode(df[column_name].to_list(), convert_to_tensor=False, show_progress_bar=True)
    
    # Convert embeddings to a DataFrame.
    embeddings_df = pd.DataFrame(embeddings)
    
    # Rename the columns to indicate they are embedding dimensions.
    embeddings_df.columns = [f'embedding_{i}' for i in range(embeddings_df.shape[1])]
    
    # Concatenate the original DataFrame with the embeddings DataFrame.
    result_df = pd.concat([df, embeddings_df], axis=1)
    
    return result_df

```

# Quick Example: 

Let's grab some data from Huggingface's datasets. I'm just grabbing some emotion utterances and for this example I dont need a ton of them, so I'm just grabbing the test set to have something to work with.  

```{python}
from datasets import load_dataset

dataset = load_dataset("dair-ai/emotion", trust_remote_code=True, split='test')
df = pd.DataFrame(dataset)
```

This just lets us access the python data in R. 
```{r}
r_df <- py$df
```
Just as an intermediate look, let's just grab some embeddings. 

```{r}
look_at_embeddings <- add_embeddings_to_dataframe(r_df, 'text')
inspect <- look_at_embeddings |> 
  select(1:12) 

gt(inspect) |> 
  tab_options(
    container.height = px(400)
  )
```

Here is how we can align some set of labels to each row of utterances/verbatims. 

```{r}
# Create some arbitrary labels: 
labels <- tibble(
  label = c('mad', 'sad', 'interested', 'happy', 'doesnt care'))

classification_probs <- embedding_cosign(cats = labels, input_data = r_df, target_verbatim = 'text') |> 
  arrange(-mad)
```

```{r}
gt(classification_probs) |> 
  tab_options(
    container.height = px(400)
  )
```

Overall, once you have your scripts in place and your conda environment situated, it's easy to compute cosine (it's quick with some matrix algebra) and align your labels to each row of your open-ended utterances or whatever else you're interested in aligning. On my computer without any GPU assistance, the embeddings are generated around 50/second. The cosign computation is maybe less than a couple seconds for about 5k rows and 5 labels. 
