{
  "hash": "fd4ed01267973eaf8955ca595b3828e3",
  "result": {
    "markdown": "---\ntitle: 'Simulation: Part 0'\ndate: '2023-11-04'\n\ncategories: \n  - Code\n  - Simulation\n  - R\n\noutput: \n  html_document:\n    theme: journal\n    highlight: cosmo\n    \nengine: knitr\nembed-resources: true\n---\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: pacman\n```\n:::\n:::\n\n\nThis blog post will be the first in a series that will explore how social scientists, or at least those that tend to span across and into more traditional data science, can leverage and employ simulation techniques into their workflow. At the least, this series will introduce the reader to what statistical simulation is, the very basics, and how it can be used to better understand how a variety of statistical methods function across broad data situations. \n\n# What is statistical simulation and why should you care? \n\nIn our case, it's the process of making fake data in a programmatic way. Statistical simulation is a powerful tool that allows us to create and analyze data in a controlled environment. By simulating data, we can explore the behavior of statistical models under various conditionsâ€”conditions that may be rare or difficult to observe in the real world. This is particularly useful when working with complex models or when trying to understand the impact of different data characteristics on statistical inference.\n\nFor instance, if I wanted to understand how well a simple linear regression (OLS) functioned across different, often anticipated, data situations, I could simulate those situations (the data) and then compute how well it did. \n\n\nUsing that as a simple example, lets do just that:\n\n\n$$\ny = \\beta_0 + x_1\\beta_1 + \\varepsilon\n$$\nIn its simplest form, some variable $x_1$ is linearly related to $y$ given some constant influence of $\\beta_1$. Said another way, \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nset.seed(123) # For reproducibility\nn <- 10000\nbeta_0 <- 1.5\nbeta_1 <- 2.0\nsigma <- 1.0\n\nx_1 <- rnorm(n, mean = 2.5, sd = 1.0)\nepsilon <- rnorm(n, mean = 0, sd = sigma)\ny <- beta_0 + beta_1 * x_1 + epsilon\n\nsimulated_data <- tibble(x_1 = x_1, y = y)\n```\n:::\n\n\nTo get a better sense of this relationship, it's always great to visualize it. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(simulated_data, aes(x = x_1, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = beta_0, slope = beta_1, color = \"blue\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Simulated Data for OLS\",\n       x = \"Independent Variable (x_1)\",\n       y = \"Dependent Variable (y)\")\n```\n\n::: {.cell-output-display}\n![](simulation_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNow we can estimate a model. \n\n::: {.cell}\n\n```{.r .cell-code}\nols_model <- lm(y ~ x_1, data = simulated_data)\nsummary(ols_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x_1, data = simulated_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4826 -0.6689 -0.0074  0.6809  3.7696 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.47581    0.02698    54.7   <2e-16 ***\nx_1          2.00604    0.01003   200.0   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.002 on 9998 degrees of freedom\nMultiple R-squared:    0.8,\tAdjusted R-squared:    0.8 \nF-statistic: 4e+04 on 1 and 9998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n \nGiven the model, we can now compare how well our model fits the conditions for which we simulated the data. Given the variability we weaved in, you can see that we approximate the parameters we established within the simulation function above. \n\n# Utility of Simulation \n\nWe can adjust the simulation parameters to see how well any method, in this case OLS, handles new parameters that change how the data interacts. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma_new <- 2.0\nepsilon_new <- rnorm(n, mean = 0, sd = sigma_new)\ny_new <- beta_0 + beta_1 * x_1 + epsilon_new\n\nsimulated_data_new <- tibble(x_1 = x_1, y = y_new)\nols_model_new <- lm(y ~ x_1, data = simulated_data_new)\nsummary(ols_model_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x_1, data = simulated_data_new)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9672 -1.3655 -0.0144  1.3816  8.7195 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.38569    0.05389   25.71   <2e-16 ***\nx_1          2.04008    0.02003  101.83   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.001 on 9998 degrees of freedom\nMultiple R-squared:  0.5091,\tAdjusted R-squared:  0.5091 \nF-statistic: 1.037e+04 on 1 and 9998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nIn this case, we can compare the models to see how well OLS handles an increase in error variance. \n\n# Next Steps\nIn the next post, we'll look at how we can simulation non-linear relationships. Stay tuned. \n\n# Python Version\nIf you're curious, this would be the python rendition\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Parameters\nn = 10000\nbeta_0 = 1.5\nbeta_1 = 2.0\nsigma = 1.0\n\n# Simulate independent variable x_1\nx_1 = np.random.normal(2.5, 1.0, n)\n# Simulate error term\nepsilon = np.random.normal(0, sigma, n)\n# Simulate dependent variable y\ny = beta_0 + beta_1 * x_1 + epsilon\n\n# Plot the simulated data and the true regression line\nplt.figure(figsize=(10, 6))\nplt.scatter(x_1, y, alpha=0.5, label='Simulated data')\nplt.plot(x_1, beta_0 + beta_1 * x_1, 'r', label='True regression line')\nplt.title('Simulated Data for OLS')\nplt.xlabel('Independent Variable (x_1)')\nplt.ylabel('Dependent Variable (y)')\nplt.legend()\nplt.show()\n\n# Add a constant to the independent variable to model the intercept\nx_1_with_const = sm.add_constant(x_1)\n\n# Fit OLS regression model\nmodel = sm.OLS(y, x_1_with_const)\nresults = model.fit()\n\n# Print out the statistics\nprint(results.summary())\n```\n:::\n",
    "supporting": [
      "simulation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}