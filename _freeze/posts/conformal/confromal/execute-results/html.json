{
  "hash": "957f3ff98bd64f0a698ca2c28b22b18f",
  "result": {
    "markdown": "---\ntitle: \"Conformal Prediction\"\ndate: '2024-07-30'\ncategories: [R, Conformal]\n\noutput: \n  html_document:\n    theme: journal\n    highlight: zenburn\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Description of file here\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: pacman\n```\n:::\n\n```{.r .cell-code}\npacman::p_load(tidyverse, explore, tidylog, skimr, plotly, \n               performance, mgcv, quantregForest,\n               gridExtra)\n```\n:::\n\n\nConformal prediction stands at the forefront of modern data analysis, offering a robust way to assess the reliability of predictive models. This technique is particularly valuable in scenarios where accurate and reliable predictions are crucial, such as in healthcare, finance, and various other business cases. In most real work cases, the error of any given model is not uniform or constant across the range of possible predictions. Therefore, it's important to take this into account when using a model to predict.\n\nConformal prediction is notably non-parametric and distribution-free, meaning it is not constrained by the underlying distribution of your data. This characteristic lends it exceptional flexibility, making it applicable across various types of models. It is effective for both classification and regression problems, regardless of the specific model used.\n\nDespite aligning with the frequentist perspective in quantifying uncertainty, conformal prediction provides a robust guarantee regarding the error bounds. It ensures that these bounds will encompass the true outcome within a specified confidence level. This makes it a reliable tool for making predictions with quantifiable certainty.\n\nThis post will cover a few different ways to compute prediction intervals.\n\n# Data Simulation:\n\nLets start by generating data often found in the real world. These data will produce a probability that ranges from 0 to 1. This will let me flip this to a classification problem later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)  # for reproducibility\n\n# Simulating the dataset\nn <- 10000  # number of observations\ndata <- tibble(\n  age = runif(n, 18, 70),  # random ages between 18 and 70\n  income = runif(n, 30000, 100000),  # random income between 30k and 100k\n)\n\n# Create a non-linear relationship for the target variable\nsim_data <- data %>%\n  mutate(\n    purchase_likelihood = 0.5 * sin(age / 10) + 0.3 * log(income / 30000) + rnorm(n, 0, 0.2),\n    purchase = as.factor(ifelse(purchase_likelihood > 0.5, 1, 0))  # binary target variable\n  )\n```\n:::\n\n\nAnother way:\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_data <- function(n, base_std_dev = 1 / 2) {\n  tibble(x = runif(n, min = -2, max = 2)) %>% # Ensuring x ranges between -2 and 2\n    mutate(\n      y = (x^4) + 2 * exp(-7 * (x - 0.3)^2),\n      y = y + rnorm(n, sd = base_std_dev * (1 + abs(x^3))) # Varying std_dev with x\n    )\n}\n\nn <- 10000\nset.seed(8383)\nsim_data <- make_data(n)\n\n\nsim_data %>% \n  ggplot(aes(x, y)) + \n  geom_point(alpha = 1 / 10)\n```\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# This is from the tidymodels example here: https://www.tidymodels.org/learn/models/conformal-regression/\n# \n\nmake_variable_data <- function(n, std_dev = 1 / 5) {\n  tibble(x = runif(n, min = -1)) %>%\n    mutate(\n      y = (x^3) + 2 * exp(-6 * (x - 0.3)^2),\n      y = y + rnorm(n, sd = std_dev * abs(x))\n    )\n}\n\nn <- 10000\nset.seed(8383)\nsim_data <- make_variable_data(n)\n\nsim_data %>% \n  ggplot(aes(x, y)) + \n  geom_point(alpha = 1 / 10)\n```\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nCool, let's see how well a few simple models do.\n\nBut first, we must split it up:\n\nThis is important for common model performance checking, but also because we'll need the validation set for conformal prediction later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit <- rsample::initial_validation_split(sim_data)\ntrain <- rsample::training(split)\ntest <- rsample::testing(split)\nval <- rsample::validation(split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_mod <- lm(y ~ x, data = train)\nspline_mod <- gam(y ~ s(x), data = train)\n\ncompare_performance(lm_mod, spline_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Comparison of Model Performance Indices\n\nName       | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 |  RMSE | Sigma | R2 (adj.)\n------------------------------------------------------------------------------------------------------------\nlm_mod     |    lm |  9495.3 (<.001) |  9495.3 (<.001) |  9515.4 (<.001) | 0.642 | 0.534 | 0.534 |     0.642\nspline_mod |   gam | -8746.0 (>.999) | -8745.9 (>.999) | -8672.6 (>.999) | 0.983 | 0.117 | 0.117 |          \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_preds <- test %>% \n  mutate(lm_predictions = predict(lm_mod, newdata = .),\n         spline_predictions = predict(spline_mod, newdata = .))\n\nlm_metrics <- test_preds %>% \n  yardstick::metrics(truth = y, estimate = lm_predictions) %>% \n  mutate(type = 'lm_metrics')\nspline_metrics <- test_preds %>% \n  yardstick::metrics(truth = y, estimate = spline_predictions) %>% \n  mutate(type = 'spline_metrics')\n\nperf <- bind_rows(lm_metrics, spline_metrics)\nperf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  .metric .estimator .estimate type          \n  <chr>   <chr>          <dbl> <chr>         \n1 rmse    standard      0.548  lm_metrics    \n2 rsq     standard      0.624  lm_metrics    \n3 mae     standard      0.444  lm_metrics    \n4 rmse    standard      0.118  spline_metrics\n5 rsq     standard      0.983  spline_metrics\n6 mae     standard      0.0822 spline_metrics\n```\n:::\n:::\n\n\nCool, let's see how this looks over the range of test truth and estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_plot <- test_preds %>% \n  ggplot(aes(x = x, y = lm_predictions)) + \n  geom_point() +\n  theme_minimal()\n\nspline_plot <- test_preds %>% \n  ggplot(aes(x = x, y = spline_predictions)) + \n  geom_point() +\n  theme_minimal()\n\ngrid.arrange(lm_plot, spline_plot, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nOk, let's build out the basic steps for conformal prediction:\n\n1.  Data Split: We've already done this earlier. The validation set, or calibration as some people call it, is important in this process. Make sure you have that.\n\n2.  Model Training: Train a model. In this case, we'll train both the linear model and the GAM.\n\n3.  Conformity Measure: We'll need to define a conformity measure. This is a function that essentially assigns a numerical score to each instance, which reflects how well the estimate 'conforms' to the other instances in the validation set. Typically this is just the 'distance' or error between your model's prediction and the validation set. However, this could be anything for your specific problem.\n\n4.  Conformity Score: This is the computation of error given the measure in step 3.\n\n5.  Prediction: For any new instance that you'd like to predict, you use your model to run prediction and then we'll move to compute a conformity score.\n\n6.  Confidence Level Determination: Given your new instances conformity score, we'll now compare it to the distribution of scores derived from the validation set to determine the confidence level.\n\n7.  Output Prediction Intervals: We can then generate a prediction interval.\n\nIn its simplest form, conformal prediction is a statistical technique that provides a measure of certainty for machine learning model predictions by generating prediction intervals, using the conformity of new instances to a calibration set to indicate how likely these predictions are to be accurate. This method leverages the distribution of conformity scores from a calibration set to assess the reliability of predictions for new data.\n\nBecause we've already trained our two models, we now just need to leverage the validation set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nlm_validation_scores <- val %>%\n  mutate(\n    prediction = predict(lm_mod, .),\n    error = abs(y - prediction)\n  )\n\nquant <- quantile(lm_validation_scores$error, probs = 0.95)\n\nlm_validation_scores %>% \n  ggplot(aes(x = error)) + \n  geom_histogram() + \n  geom_vline(xintercept = quant)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspline_validation_scores <- val %>%\n  mutate(\n    prediction = predict(spline_mod, .),\n    error = abs(y - prediction)\n  )\n\nquant <- quantile(spline_validation_scores$error, probs = 0.95)\n\nspline_validation_scores %>% \n  ggplot(aes(x = error)) + \n  geom_histogram() + \n  geom_vline(xintercept = quant)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n# Split Conformal Intervals\n\nNow we can create a function that uses all the information we have to generate conformal intervals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate the conformal prediction interval for new data without y values\nconformal_interval <- function(new_data, model, calibration_set, alpha = 0.95) {\n  # Predict y values for the calibration set\n  validation_scores <- calibration_set %>%\n    mutate(\n      prediction = predict(model, .),\n      error = abs(y - prediction)\n    )\n  \n  error_quantile <- quantile(validation_scores$error, probs = 0.95)\n  print(error_quantile)\n  # # Predict y values for the new data\n  \n  new_preds <- new_data %>% \n    mutate(predictions = predict(model, .),\n           .lower = predictions - error_quantile,\n           .upper = predictions + error_quantile\n           )\n}\n\nlm_result <- conformal_interval(new_data = test, model = lm_mod, calibration_set = val, alpha = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      95% \n0.9635453 \n```\n:::\n\n```{.r .cell-code}\nlm_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,000 × 5\n         x       y predictions   .lower .upper\n     <dbl>   <dbl>       <dbl>    <dbl>  <dbl>\n 1  0.197   1.89        0.960  -0.00344  1.92 \n 2 -0.265   0.164       0.389  -0.575    1.35 \n 3 -0.523  -0.0874      0.0699 -0.894    1.03 \n 4  0.845   1.10        1.76    0.799    2.73 \n 5  0.0130  1.22        0.733  -0.231    1.70 \n 6 -0.819  -0.679      -0.297  -1.26     0.667\n 7 -0.630  -0.261      -0.0630 -1.03     0.901\n 8 -0.560  -0.0740      0.0243 -0.939    0.988\n 9 -0.314   0.166       0.329  -0.635    1.29 \n10 -0.0195  1.08        0.693  -0.271    1.66 \n# ℹ 1,990 more rows\n```\n:::\n:::\n\nNot sure this viz is useful:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_result %>% \n  slice_sample(n = 100) %>% \n  ggplot(aes(x = y, y = predictions)) + \n  geom_point() + \n  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nslice_sample: removed 1,900 rows (95%), 100 rows remaining\n```\n:::\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_result %>%\n  ggplot(aes(x = x, y = predictions)) +\n  geom_point(color = \"black\", size = 0.5) + # Adjust point color and size\n  geom_point(aes(x = x, y = y), color = 'green', size = 0.5) +\n  geom_errorbar(aes(ymin = .lower, ymax = .upper),\n    width = 0.2, # Adjust the width of the error bars\n    color = \"grey\", # Change color of error bars\n    alpha = 0.2\n  ) + # Adjust transparency\n  ggtitle(\"Age vs Predictions with Conformal Prediction Intervals\") + # Add a title\n  labs(x = \"Age\", y = \"Predicted Value\") + # Label the axes\n  theme_minimal() + # Apply a minimal theme\n  theme(\n    plot.title = element_text(hjust = 0.5), # Center the title\n    text = element_text(size = 12)\n  ) # Adjust text size\n```\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nAs you can see, the relationship between age and the prediction is actually non-linear (as designed/simulated, in green), however, what is also obvious is that the error across the range of age is also not normal and changes as a function of the predictor.\n\nSo, given the modeled linear effect, this seem plausible.\n\nLet's take a look at the spline model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspline_result <- conformal_interval(new_data = test, model = spline_mod, calibration_set = val, alpha = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      95% \n0.2540535 \n```\n:::\n:::\n\nNot sure this is useful:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspline_result %>% \n  # slice_sample(n = 100) %>% \n  ggplot(aes(x = y, y = predictions)) + \n  geom_point() + \n  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.1)\n```\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspline_result %>%\n  slice_sample(n = 1000) %>% \n  ggplot(aes(x = x, y = predictions)) +\n  geom_point(color = \"black\", size = 0.5) + # Adjust point color and size\n  geom_point(aes(x = x, y = y), color = 'green', size = 0.5) +\n  geom_errorbar(aes(ymin = .lower, ymax = .upper),\n    width = 0.05, # Adjust the width of the error bars\n    color = \"grey\", # Change color of error bars\n    alpha = 0.2\n  ) + # Adjust transparency\n  ggtitle(\"X vs Predictions with Conformal Prediction Intervals\") + # Add a title\n  labs(x = \"Age\", y = \"Predicted Value\") + # Label the axes\n  theme_minimal() + # Apply a minimal theme\n  theme(\n    plot.title = element_text(hjust = 0.5), # Center the title\n    text = element_text(size = 12)\n  ) # Adjust text size\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nslice_sample: removed 1,000 rows (50%), 1,000 rows remaining\n```\n:::\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspline_result %>% \n  slice_sample(n = 2000) %>% \n  ggplot(aes(x)) +\n  geom_point(aes(y = predictions), color = 'blue') +\n  geom_point(aes(y = y), alpha = 1/10) + \n  geom_ribbon(aes(ymin = .lower, ymax = .upper), \n              col = \"#D95F02\", linewidth = 3 / 4, fill = NA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nslice_sample: no rows removed\n```\n:::\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nAlso plausible. But in truth, there is nothing super fancy about estimating error from the 95% quantile perspective. In other words, we assume that all the error for each point that could be predicted might be found in this span of error if we did this forever. Of course, this is a frequentist perspective. While I personally dont always ascribe to this perspective, I think it works fine. The only thing that is of issue here is that we obviously know there are some prediction values we can predict better than others simply given the data arrangement and how some multivariate combinations of predictors arrive at a stronger prediction.\n\n# Conformalized Quantile Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconformal_quantile_intervals <- function(train_set, calib_set, new_data, alpha = 0.05) {\n  # Fit the quantile regression tree model\n  train_x <- train_set %>% select(-y)\n  train_y <- train_set %>% select(y) %>% pull()\n  \n  qrf_model <- quantregForest(x = train_x, \n                              y = train_y,\n                              nthreads = 8)\n  \n  # Determine the lower and upper quantiles\n  quantiles <- c(alpha/2, 1 - alpha/2)\n  \n  # Predict quantiles on the calibration set\n  calib_pred <- predict(qrf_model, calib_set[, -ncol(calib_set)], \n                        type = \"quantiles\", quantiles = quantiles)\n  \n  # Compute conformity scores\n  lower_bound <- calib_pred[, 1]\n  upper_bound <- calib_pred[, 2]\n  calib_actual <- calib_set[, ncol(calib_set)]\n  conformity_scores <- pmax(lower_bound - calib_actual,\n                            calib_actual - upper_bound)\n\n  # Calculate the quantile of the conformity scores\n  q_alpha <- quantile(conformity_scores$y, probs = 1 - alpha)\n\n  # Predict quantiles for new data\n  new_pred <- predict(qrf_model, new_data, type = \"quantiles\",\n                      quantiles = quantiles)\n\n  # Adjust prediction intervals based on conformity scores\n  intervals <- cbind(new_pred[, 1] - q_alpha, new_pred[, 2] + q_alpha)\n  colnames(intervals) <- c(\"quant_lower\", \"quant_upper\")\n\n  return(intervals)\n}\n\nconformal_quant_intervals <- conformal_quantile_intervals(\n  train_set = train, \n  calib_set = val,\n  new_data = test,\n  alpha = 0.05\n)\n```\n:::\n\n\nLet's visualize this! For the predictions, we'll just use the point predictions generated from the spline model... as it was the best looking. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_quant_int <- bind_cols(spline_result, conformal_quant_intervals)\n\nconf_quant_int %>%\n  slice_sample(n = 1000) %>% \n  ggplot(aes(x = x, y = predictions)) +\n  geom_point(color = \"black\", size = 0.5) + # Adjust point color and size\n  geom_point(aes(x = x, y = y), color = 'green', size = 0.5) +\n  geom_errorbar(aes(ymin = quant_lower, ymax = quant_upper),\n    width = 0.05, # Adjust the width of the error bars\n    color = \"grey\", # Change color of error bars\n    alpha = 0.2\n  ) + # Adjust transparency\n  ggtitle(\"X vs Predictions with Conformal Prediction Intervals\") + # Add a title\n  labs(x = \"x\", y = \"Predicted Value\") + # Label the axes\n  theme_minimal() + # Apply a minimal theme\n  theme(\n    plot.title = element_text(hjust = 0.5), # Center the title\n    text = element_text(size = 12)\n  ) # Adjust text size\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nslice_sample: removed 1,000 rows (50%), 1,000 rows remaining\n```\n:::\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nAlternative visualization\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_quant_int %>% \n  # slice_sample(n = 500) %>% \n  ggplot(aes(x)) +\n  geom_point(aes(y = predictions), color = 'blue') +\n  geom_point(aes(y = y), alpha = 1/10) + \n  geom_ribbon(aes(ymin = quant_lower, ymax = quant_upper), \n              col = \"#D95F02\", linewidth = 3 / 4, fill = NA)\n```\n\n::: {.cell-output-display}\n![](confromal_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nThis is a work in progress... \n",
    "supporting": [
      "confromal_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}